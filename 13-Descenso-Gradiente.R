# Descenso del Gradiente

# ---------------------------------------------------
# 1. EjecuciÃ³n del algoritmo de descenso de gradiente
# 2. VisualizaciÃ³n e interpretaciÃ³n de resultados
# ---------------------------------------------------

# Recuerde: si F es una funciÃ³n de x e y, entonces f_x y f_y representan las derivadas parciales de f con respecto a x e y.
# Las derivadas parciales son funciones que describen la tasa de cambio de f, en las direcciones x e y.

# Entonces podemos definir una funciÃ³n de ğ‘“_ğ‘¥ y ğ‘“_ğ‘¦.
# Esta funciÃ³n, conocida como ğ›»ğ‘“  es el gradiente de ğ‘“.
# ğ›»ğ‘“ (ğ‘¥, ğ‘¦) = [ğ‘“_ğ‘¥ (ğ‘¥, ğ‘¦), ğ‘“_ğ‘¦ (ğ‘¥, ğ‘¦)] es una funciÃ³n vectorial. Devuelve un vector.
# ğ›»ğ‘“ proporciona la direcciÃ³n con la tasa mÃ¡xima de cambio de ğ‘“ desde el punto (ğ‘¥, ğ‘¦)

# Algoritmo del Descenso del Gradiente:

# Imagina que estÃ¡s parado en la ladera de una montaÃ±a y deseas caminar hasta el fondo. Sin embargo, tiene los ojos vendados y no puede ver en quÃ© direcciÃ³n caminar.
# Al probar el paso (pendiente) de todas las direcciones a su alrededor, puede seleccionar la direcciÃ³n que desciende mÃ¡s rÃ¡pidamente.
# Cada pocos pasos, se detiene y revisa todas las pendientes circundantes nuevamente, y ajusta el rumbo segÃºn sea necesario.
# El descenso de gradientes funciona exactamente asÃ­, pero desde un punto de partida en una curva hasta el mÃ­nimo (o mÃ¡ximo).


# Algoritmo del Descenso del Gradiente:

# Definir ğ›»ğ‘“ (ğ‘¥, ğ‘¦) = [ğ‘“_ğ‘¥ (ğ‘¥, ğ‘¦), ğ‘“_ğ‘¦ (ğ‘¥, ğ‘¦)]
# Inicializar un punto de partida (ğ‘¥_1, ğ‘¦_1)
# Calcular ğ›»ğ‘“ (ğ‘¥_1, ğ‘¦_1)
# Calcule (ğ‘¥_2, ğ‘¦_2) como ğ‘¥_2 = ğ‘¥_1 âˆ’ ğ›¼ âˆ— ğ‘“_ğ‘¥ (ğ‘¥_1, ğ‘¦_1) y ğ‘¦_2 = ğ‘¦_1 âˆ’ ğ›¼ âˆ— ğ‘“_ğ‘¦ (ğ‘¥_1, ğ‘¦_1)
# ğ›¼ es la tasa de aprendizaje y determina quÃ© tan grandes son los pasos
# Repita hasta que (ğ‘¥_ğ‘›, ğ‘¦_ğ‘›) sea un punto crÃ­tico



library(Deriv)
library(rgl)

# ---------------------------------------------------
# 1. EjecuciÃ³n del algoritmo de descenso de gradiente
# ---------------------------------------------------

# 0. Defina y grafique f (x, y) = (x-2)^2 + (y+3)^2 para xey entre -10 y 10

f <- function(x,y) (x-2)^2 + (y+3)^2

plot3d(f, xlim = c(-10,10), ylim = c(-10,10), col = 'green')

## 1. Define el gradiente de f

f.x <- Deriv(f, x = 'x') ## derivada parcial con respecto a x
f.y <- Deriv(f, x = 'y') ## derivada parcial con respecto a y

# 2. Inicializar un punto de partida
x <- 0
y <- 0

### Ejecutar un bucle para repetir la secciÃ³n
alpha = 0.01
n.iter <- 500
f.history <- numeric(n.iter)
x.history <- numeric(n.iter)
y.history <- numeric(n.iter)

for (i in 1:n.iter) {
  # 3. Calcular el gradiente (derivadas parciales evaluado en el punto) 
  x.gradient <- f.x(x=x)
  y.gradient <- f.y(y=y)
  
  # 4. Calcule el nuevo punto como xnew = x - alpha * f_x (x, y) y ynew = y - alpha * f_y (x, y)
  x <- x - alpha*x.gradient
  y <- y - alpha*y.gradient
  
  # 5. Repita hasta que (x, y) sea un punto crÃ­tico
  
  f.history[i] <- f(x,y)
  x.history[i] <- x
  y.history[i] <- y
}



# ---------------------------------------------------
# 2. VisualizaciÃ³n e interpretaciÃ³n de resultados
# ---------------------------------------------------
plot(f.history)
plot(x.history, y.history)

plot3d(f, xlim = c(-5,5), ylim = c(-5,5), col = 'green')
points3d(x.history, y.history, f(x.history,y.history), col = 'red')








